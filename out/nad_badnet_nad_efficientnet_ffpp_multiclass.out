/home/fmg/yuran/miniconda3/envs/backdoorbenchv2/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
WARNING:root:save_path MUST have 'record' in its abspath, and data_path in attack result MUST have 'data' in its path
WARNING:root:For ImageNet, this script need large size of RAM to load the whole dataset.
WARNING:root:save_path MUST have 'record' in its abspath, and data_path in attack result MUST have 'data' in its path
WARNING:root:For ImageNet, this script need large size of RAM to load the whole dataset.
INFO:root:{'amp': True,
 'batch_size': 256,
 'beta1': 500,
 'beta2': 1000,
 'beta3': 1000,
 'checkpoint_load': None,
 'checkpoint_save': 'record/nad_badnet_attack_efficientnet_ffpp_multiclass/defense/nad/checkpoint/',
 'client_optimizer': 'sgd',
 'dataset': 'ffpp_multiclass',
 'dataset_path': './data/ffpp_multiclass',
 'device': 'cuda',
 'epochs': 100,
 'frequency_save': 0,
 'img_size': (64, 64, 3),
 'index': None,
 'input_channel': 3,
 'input_height': 64,
 'input_width': 64,
 'log': 'record/nad_badnet_attack_efficientnet_ffpp_multiclass/defense/nad/log/',
 'lr': 0.01,
 'lr_scheduler': 'CosineAnnealingLR',
 'model': 'efficientnet_b3',
 'momentum': 0.9,
 'non_blocking': True,
 'num_classes': 6,
 'num_workers': 4,
 'p': 2.0,
 'pin_memory': True,
 'prefetch': False,
 'random_seed': 0,
 'ratio': 0.05,
 'result_file': 'nad_badnet_attack_efficientnet_ffpp_multiclass',
 'save_path': 'record/nad_badnet_attack_efficientnet_ffpp_multiclass/defense/nad/',
 'sgd_momentum': 0.9,
 'te_epochs': 10,
 'teacher_model_loc': None,
 'terminal_info': ['./defense/nad.py',
                   '--yaml_path',
                   './config/defense/nad/cifar10.yaml',
                   '--model',
                   'efficientnet_b3',
                   '--dataset',
                   'ffpp_multiclass',
                   '--result_file',
                   'nad_badnet_attack_efficientnet_ffpp_multiclass'],
 'wd': 0.0005,
 'weight_decay': 0.0001,
 'yaml_path': './config/defense/nad/cifar10.yaml'}
2025-03-19:03:02:19 [INFO    ] [nad.py:798] {'amp': True,
 'batch_size': 256,
 'beta1': 500,
 'beta2': 1000,
 'beta3': 1000,
 'checkpoint_load': None,
 'checkpoint_save': 'record/nad_badnet_attack_efficientnet_ffpp_multiclass/defense/nad/checkpoint/',
 'client_optimizer': 'sgd',
 'dataset': 'ffpp_multiclass',
 'dataset_path': './data/ffpp_multiclass',
 'device': 'cuda',
 'epochs': 100,
 'frequency_save': 0,
 'img_size': (64, 64, 3),
 'index': None,
 'input_channel': 3,
 'input_height': 64,
 'input_width': 64,
 'log': 'record/nad_badnet_attack_efficientnet_ffpp_multiclass/defense/nad/log/',
 'lr': 0.01,
 'lr_scheduler': 'CosineAnnealingLR',
 'model': 'efficientnet_b3',
 'momentum': 0.9,
 'non_blocking': True,
 'num_classes': 6,
 'num_workers': 4,
 'p': 2.0,
 'pin_memory': True,
 'prefetch': False,
 'random_seed': 0,
 'ratio': 0.05,
 'result_file': 'nad_badnet_attack_efficientnet_ffpp_multiclass',
 'save_path': 'record/nad_badnet_attack_efficientnet_ffpp_multiclass/defense/nad/',
 'sgd_momentum': 0.9,
 'te_epochs': 10,
 'teacher_model_loc': None,
 'terminal_info': ['./defense/nad.py',
                   '--yaml_path',
                   './config/defense/nad/cifar10.yaml',
                   '--model',
                   'efficientnet_b3',
                   '--dataset',
                   'ffpp_multiclass',
                   '--result_file',
                   'nad_badnet_attack_efficientnet_ffpp_multiclass'],
 'wd': 0.0005,
 'weight_decay': 0.0001,
 'yaml_path': './config/defense/nad/cifar10.yaml'}
INFO:root:{'git hash': None,
 'last 3 log': 'commit a8181b8b94f067ae63aa75661c991e3fba336c84\n'
               'Author: QiuMatthew <uzenkyu@gmail.com>\n'
               'Date:   Wed Mar 19 02:22:11 2025 +0900\n'
               '\n'
               '    add loggin for debug\n'
               '\n'
               'commit 0464ef2c0b243f6dc6d9dc5f5c0031d935bed08a\n'
               'Author: QiuMatthew <q_masio@outlook.com>\n'
               'Date:   Wed Mar 19 00:52:41 2025 +0900\n'
               '\n'
               '    print result of activation map\n'
               '\n'
               'commit f628a0afea8aece966aa3831e27b18fb9af31ee5\n'
               'Author: QiuMatthew <uzenkyu@gmail.com>\n'
               'Date:   Tue Mar 18 23:35:06 2025 +0900\n'
               '\n'
               '    comment out saving process to save time',
 'status': 'On branch nad\n'
           "Your branch is up to date with 'origin/nad'.\n"
           '\n'
           'Changes not staged for commit:\n'
           '  (use "git add <file>..." to update what will be committed)\n'
           '  (use "git restore <file>..." to discard changes in working '
           'directory)\n'
           '\tmodified:   out/nad_badnet_nad_efficientnet_ffpp_multiclass.out\n'
           '\n'
           '\n'
           "It took 2.69 seconds to enumerate untracked files. 'status -uno'\n"
           'may speed it up, but you have to be careful not to forget to add\n'
           "new files yourself (see 'git help status').\n"
           'no changes added to commit (use "git add" and/or "git commit -a")'}
2025-03-19:03:02:25 [INFO    ] [nad.py:801] {'git hash': None,
 'last 3 log': 'commit a8181b8b94f067ae63aa75661c991e3fba336c84\n'
               'Author: QiuMatthew <uzenkyu@gmail.com>\n'
               'Date:   Wed Mar 19 02:22:11 2025 +0900\n'
               '\n'
               '    add loggin for debug\n'
               '\n'
               'commit 0464ef2c0b243f6dc6d9dc5f5c0031d935bed08a\n'
               'Author: QiuMatthew <q_masio@outlook.com>\n'
               'Date:   Wed Mar 19 00:52:41 2025 +0900\n'
               '\n'
               '    print result of activation map\n'
               '\n'
               'commit f628a0afea8aece966aa3831e27b18fb9af31ee5\n'
               'Author: QiuMatthew <uzenkyu@gmail.com>\n'
               'Date:   Tue Mar 18 23:35:06 2025 +0900\n'
               '\n'
               '    comment out saving process to save time',
 'status': 'On branch nad\n'
           "Your branch is up to date with 'origin/nad'.\n"
           '\n'
           'Changes not staged for commit:\n'
           '  (use "git add <file>..." to update what will be committed)\n'
           '  (use "git restore <file>..." to discard changes in working '
           'directory)\n'
           '\tmodified:   out/nad_badnet_nad_efficientnet_ffpp_multiclass.out\n'
           '\n'
           '\n'
           "It took 2.69 seconds to enumerate untracked files. 'status -uno'\n"
           'may speed it up, but you have to be careful not to forget to add\n'
           "new files yourself (see 'git help status').\n"
           'no changes added to commit (use "git add" and/or "git commit -a")'}
INFO:root:----------- Network Initialization --------------
2025-03-19:03:02:25 [INFO    ] [nad.py:821] ----------- Network Initialization --------------
INFO:root:finished teacher student init...
2025-03-19:03:03:01 [INFO    ] [nad.py:833] finished teacher student init...
INFO:root:finished student student init...
2025-03-19:03:03:01 [INFO    ] [nad.py:845] finished student student init...
INFO:root:save file format is .png
2025-03-19:03:03:01 [INFO    ] [bd_dataset_v2.py:133] save file format is .png
INFO:root:Do NOT set the settings/parameters attr manually after you start training!
You may break the relationship between them.
2025-03-19:03:03:01 [INFO    ] [trainer_cls.py:972] Do NOT set the settings/parameters attr manually after you start training!
You may break the relationship between them.
loading...
loading...
INFO:root:('epoch_now:0, '
 'batch_now:0self.amp:True,self.criterion:CrossEntropyLoss(),self.optimizer:SGD '
 '(\n'
 'Parameter Group 0\n'
 '    dampening: 0\n'
 '    initial_lr: 0.01\n'
 '    lr: 0.01\n'
 '    maximize: False\n'
 '    momentum: 0.9\n'
 '    nesterov: False\n'
 '    weight_decay: 0.0005\n'
 "),self.scheduler:{'T_max': 100, 'eta_min': 0, 'base_lrs': [0.01], "
 "'last_epoch': 0, '_step_count': 1, 'verbose': False, "
 "'_get_lr_called_within_step': False, '_last_lr': "
 "[0.01]},self.scaler:{'scale': 65536.0, 'growth_factor': 2.0, "
 "'backoff_factor': 0.5, 'growth_interval': 2000, '_growth_tracker': 0})")
2025-03-19:03:03:02 [INFO    ] [trainer_cls.py:1030] ('epoch_now:0, '
 'batch_now:0self.amp:True,self.criterion:CrossEntropyLoss(),self.optimizer:SGD '
 '(\n'
 'Parameter Group 0\n'
 '    dampening: 0\n'
 '    initial_lr: 0.01\n'
 '    lr: 0.01\n'
 '    maximize: False\n'
 '    momentum: 0.9\n'
 '    nesterov: False\n'
 '    weight_decay: 0.0005\n'
 "),self.scheduler:{'T_max': 100, 'eta_min': 0, 'base_lrs': [0.01], "
 "'last_epoch': 0, '_step_count': 1, 'verbose': False, "
 "'_get_lr_called_within_step': False, '_last_lr': "
 "[0.01]},self.scaler:{'scale': 65536.0, 'growth_factor': 2.0, "
 "'backoff_factor': 0.5, 'growth_interval': 2000, '_growth_tracker': 0})")
INFO:root:one epoch training part done, use time = 41.855851888656616 s
2025-03-19:03:03:43 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 41.855851888656616 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.004673208509173,
 'clean_test_loss_avg_over_batch': 1.4218136325027004,
 'epoch': 0,
 'test_acc': 0.5866666666666667,
 'test_asr': 0.08557142857142858,
 'test_ra': 0.621,
 'train_acc': 0.8187962962962962,
 'train_epoch_loss_avg_over_batch': 0.5165045296444613}
2025-03-19:03:04:18 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.004673208509173,
 'clean_test_loss_avg_over_batch': 1.4218136325027004,
 'epoch': 0,
 'test_acc': 0.5866666666666667,
 'test_asr': 0.08557142857142858,
 'test_ra': 0.621,
 'train_acc': 0.8187962962962962,
 'train_epoch_loss_avg_over_batch': 0.5165045296444613}
INFO:root:one epoch training part done, use time = 28.40243434906006 s
2025-03-19:03:04:47 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 28.40243434906006 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.893561226981027,
 'clean_test_loss_avg_over_batch': 1.7807242545214566,
 'epoch': 1,
 'test_acc': 0.5661904761904762,
 'test_asr': 0.056428571428571425,
 'test_ra': 0.6278571428571429,
 'train_acc': 0.8585648148148148,
 'train_epoch_loss_avg_over_batch': 0.37895960281876956}
2025-03-19:03:05:04 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.893561226981027,
 'clean_test_loss_avg_over_batch': 1.7807242545214566,
 'epoch': 1,
 'test_acc': 0.5661904761904762,
 'test_asr': 0.056428571428571425,
 'test_ra': 0.6278571428571429,
 'train_acc': 0.8585648148148148,
 'train_epoch_loss_avg_over_batch': 0.37895960281876956}
INFO:root:one epoch training part done, use time = 21.005542755126953 s
2025-03-19:03:05:25 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 21.005542755126953 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.878158177648272,
 'clean_test_loss_avg_over_batch': 1.4021746209173491,
 'epoch': 2,
 'test_acc': 0.6148809523809524,
 'test_asr': 0.03742857142857143,
 'test_ra': 0.6901428571428572,
 'train_acc': 0.8728703703703704,
 'train_epoch_loss_avg_over_batch': 0.3364111016778385}
2025-03-19:03:05:42 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.878158177648272,
 'clean_test_loss_avg_over_batch': 1.4021746209173491,
 'epoch': 2,
 'test_acc': 0.6148809523809524,
 'test_asr': 0.03742857142857143,
 'test_ra': 0.6901428571428572,
 'train_acc': 0.8728703703703704,
 'train_epoch_loss_avg_over_batch': 0.3364111016778385}
INFO:root:one epoch training part done, use time = 24.707423210144043 s
2025-03-19:03:06:08 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 24.707423210144043 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 5.217245442526681,
 'clean_test_loss_avg_over_batch': 1.4886526519601995,
 'epoch': 3,
 'test_acc': 0.6017857142857143,
 'test_asr': 0.031285714285714285,
 'test_ra': 0.6877142857142857,
 'train_acc': 0.8880555555555556,
 'train_epoch_loss_avg_over_batch': 0.29654866527108587}
2025-03-19:03:06:23 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 5.217245442526681,
 'clean_test_loss_avg_over_batch': 1.4886526519601995,
 'epoch': 3,
 'test_acc': 0.6017857142857143,
 'test_asr': 0.031285714285714285,
 'test_ra': 0.6877142857142857,
 'train_acc': 0.8880555555555556,
 'train_epoch_loss_avg_over_batch': 0.29654866527108587}
INFO:root:one epoch training part done, use time = 20.34490704536438 s
2025-03-19:03:06:44 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 20.34490704536438 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.37795044694628,
 'clean_test_loss_avg_over_batch': 1.514100472132365,
 'epoch': 4,
 'test_acc': 0.6197619047619047,
 'test_asr': 0.1682857142857143,
 'test_ra': 0.6285714285714286,
 'train_acc': 0.895462962962963,
 'train_epoch_loss_avg_over_batch': 0.2679569249643999}
2025-03-19:03:07:00 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.37795044694628,
 'clean_test_loss_avg_over_batch': 1.514100472132365,
 'epoch': 4,
 'test_acc': 0.6197619047619047,
 'test_asr': 0.1682857142857143,
 'test_ra': 0.6285714285714286,
 'train_acc': 0.895462962962963,
 'train_epoch_loss_avg_over_batch': 0.2679569249643999}
INFO:root:one epoch training part done, use time = 26.11147379875183 s
2025-03-19:03:07:27 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 26.11147379875183 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.829375420297895,
 'clean_test_loss_avg_over_batch': 1.320073532335686,
 'epoch': 5,
 'test_acc': 0.6453571428571429,
 'test_asr': 0.091,
 'test_ra': 0.6864285714285714,
 'train_acc': 0.8922222222222222,
 'train_epoch_loss_avg_over_batch': 0.2809726892148747}
2025-03-19:03:07:47 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.829375420297895,
 'clean_test_loss_avg_over_batch': 1.320073532335686,
 'epoch': 5,
 'test_acc': 0.6453571428571429,
 'test_asr': 0.091,
 'test_ra': 0.6864285714285714,
 'train_acc': 0.8922222222222222,
 'train_epoch_loss_avg_over_batch': 0.2809726892148747}
INFO:root:one epoch training part done, use time = 23.19944190979004 s
2025-03-19:03:08:11 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 23.19944190979004 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.985055310385568,
 'clean_test_loss_avg_over_batch': 1.414970206491875,
 'epoch': 6,
 'test_acc': 0.6310714285714286,
 'test_asr': 0.10414285714285715,
 'test_ra': 0.6604285714285715,
 'train_acc': 0.90625,
 'train_epoch_loss_avg_over_batch': 0.24275571826626272}
2025-03-19:03:08:29 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.985055310385568,
 'clean_test_loss_avg_over_batch': 1.414970206491875,
 'epoch': 6,
 'test_acc': 0.6310714285714286,
 'test_asr': 0.10414285714285715,
 'test_ra': 0.6604285714285715,
 'train_acc': 0.90625,
 'train_epoch_loss_avg_over_batch': 0.24275571826626272}
INFO:root:one epoch training part done, use time = 26.514708995819092 s
2025-03-19:03:08:55 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 26.514708995819092 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.5193652766091486,
 'clean_test_loss_avg_over_batch': 1.4325800628373118,
 'epoch': 7,
 'test_acc': 0.633095238095238,
 'test_asr': 0.13057142857142856,
 'test_ra': 0.6564285714285715,
 'train_acc': 0.9035185185185185,
 'train_epoch_loss_avg_over_batch': 0.25362167744075553}
2025-03-19:03:09:17 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.5193652766091486,
 'clean_test_loss_avg_over_batch': 1.4325800628373118,
 'epoch': 7,
 'test_acc': 0.633095238095238,
 'test_asr': 0.13057142857142856,
 'test_ra': 0.6564285714285715,
 'train_acc': 0.9035185185185185,
 'train_epoch_loss_avg_over_batch': 0.25362167744075553}
INFO:root:one epoch training part done, use time = 24.022395372390747 s
2025-03-19:03:09:41 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 24.022395372390747 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.581408977508545,
 'clean_test_loss_avg_over_batch': 1.501699761910872,
 'epoch': 8,
 'test_acc': 0.6203571428571428,
 'test_asr': 0.13071428571428573,
 'test_ra': 0.6425714285714286,
 'train_acc': 0.9081944444444444,
 'train_epoch_loss_avg_over_batch': 0.2366761028766632}
2025-03-19:03:09:59 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.581408977508545,
 'clean_test_loss_avg_over_batch': 1.501699761910872,
 'epoch': 8,
 'test_acc': 0.6203571428571428,
 'test_asr': 0.13071428571428573,
 'test_ra': 0.6425714285714286,
 'train_acc': 0.9081944444444444,
 'train_epoch_loss_avg_over_batch': 0.2366761028766632}
INFO:root:one epoch training part done, use time = 27.947569370269775 s
2025-03-19:03:10:27 [INFO    ] [trainer_cls.py:1489] one epoch training part done, use time = 27.947569370269775 s
INFO:root:{'batch': 85,
 'bd_test_loss_avg_over_batch': 4.360264914376395,
 'clean_test_loss_avg_over_batch': 1.5719782366897121,
 'epoch': 9,
 'test_acc': 0.6153571428571428,
 'test_asr': 0.15514285714285714,
 'test_ra': 0.6218571428571429,
 'train_acc': 0.9146296296296297,
 'train_epoch_loss_avg_over_batch': 0.22208057887413923}
2025-03-19:03:10:33 [INFO    ] [trainer_cls.py:65] {'batch': 85,
 'bd_test_loss_avg_over_batch': 4.360264914376395,
 'clean_test_loss_avg_over_batch': 1.5719782366897121,
 'epoch': 9,
 'test_acc': 0.6153571428571428,
 'test_asr': 0.15514285714285714,
 'test_ra': 0.6218571428571429,
 'train_acc': 0.9146296296296297,
 'train_epoch_loss_avg_over_batch': 0.22208057887413923}
INFO:root:----------- Train Initialization --------------
2025-03-19:03:10:34 [INFO    ] [nad.py:914] ----------- Train Initialization --------------
INFO:root:Do NOT set the settings/parameters attr manually after you start training!
You may break the relationship between them.
2025-03-19:03:10:34 [INFO    ] [trainer_cls.py:972] Do NOT set the settings/parameters attr manually after you start training!
You may break the relationship between them.
INFO:root:('epoch_now:0, '
 'batch_now:0self.amp:True,self.criterion:CrossEntropyLoss(),self.optimizer:SGD '
 '(\n'
 'Parameter Group 0\n'
 '    dampening: 0\n'
 '    initial_lr: 0.01\n'
 '    lr: 0.01\n'
 '    maximize: False\n'
 '    momentum: 0.9\n'
 '    nesterov: False\n'
 '    weight_decay: 0.0005\n'
 "),self.scheduler:{'T_max': 100, 'eta_min': 0, 'base_lrs': [0.01], "
 "'last_epoch': 0, '_step_count': 1, 'verbose': False, "
 "'_get_lr_called_within_step': False, '_last_lr': "
 "[0.01]},self.scaler:{'scale': 65536.0, 'growth_factor': 2.0, "
 "'backoff_factor': 0.5, 'growth_interval': 2000, '_growth_tracker': 0})")
2025-03-19:03:10:34 [INFO    ] [trainer_cls.py:1030] ('epoch_now:0, '
 'batch_now:0self.amp:True,self.criterion:CrossEntropyLoss(),self.optimizer:SGD '
 '(\n'
 'Parameter Group 0\n'
 '    dampening: 0\n'
 '    initial_lr: 0.01\n'
 '    lr: 0.01\n'
 '    maximize: False\n'
 '    momentum: 0.9\n'
 '    nesterov: False\n'
 '    weight_decay: 0.0005\n'
 "),self.scheduler:{'T_max': 100, 'eta_min': 0, 'base_lrs': [0.01], "
 "'last_epoch': 0, '_step_count': 1, 'verbose': False, "
 "'_get_lr_called_within_step': False, '_last_lr': "
 "[0.01]},self.scaler:{'scale': 65536.0, 'growth_factor': 2.0, "
 "'backoff_factor': 0.5, 'growth_interval': 2000, '_growth_tracker': 0})")
INFO:root:epoch: 0  lr: 0.0100
2025-03-19:03:10:34 [INFO    ] [nad.py:89] epoch: 0  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:10:58 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:10:58 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch0: Loss:45.337091006338596 Training Acc:81.24537037037037(17549/21600)
2025-03-19:03:10:58 [INFO    ] [nad.py:622] Epoch0: Loss:45.337091006338596 Training Acc:81.24537037037037(17549/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997615814209, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.0974007323384285, at3_loss: 3.316791286067655e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999998211860657, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.07983669638633728, at3_loss: 3.733124920302089e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997615814209, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
cls_loss: 0.05466301739215851, at3_loss: 1.0019762797242038e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999998211860657, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.05618027597665787, at3_loss: 3.969047313034935e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.04119514301419258, at3_loss: 4.121702978920894e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.06079690530896187, at3_loss: 4.649058915617843e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997615814209, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 1.0
cls_loss: 0.09089351445436478, at3_loss: 1.39194211712379e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.09253589808940887, at3_loss: 6.064593272014918e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.0795418992638588, at3_loss: 3.4416913763379853e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.1830247938632965, at3_loss: 5.620504062164855e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.1054615005850792, at3_loss: 5.134781488891349e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
cls_loss: 0.10493054240942001, at3_loss: 1.2878587085651816e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2281322032213211, at3_loss: 3.733124920302089e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
cls_loss: 0.41181936860084534, at3_loss: 8.326672684688674e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.21263235807418823, at3_loss: 5.190292640122607e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2587136924266815, at3_loss: 7.41073868937292e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.3716866672039032, at3_loss: 8.465450562766819e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.5004490613937378, at3_loss: 1.0269562977782698e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.5377129316329956, at3_loss: 1.0047518372857667e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980926513672, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.4904743432998657, at3_loss: 2.538247390049264e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.6491406559944153, at3_loss: 1.774969060619469e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.6251471042633057, at3_loss: 3.069766663088558e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.4942256212234497, at3_loss: 1.9192980538207394e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.9306725859642029, at3_loss: 2.6867397195928788e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971389770508, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.7647219896316528, at3_loss: 5.881406472951767e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.6351174116134644, at3_loss: 7.363554210826351e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.8771108984947205, at3_loss: 5.936917624183025e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971389770508, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.6150146126747131, at3_loss: 8.780476346004207e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.8207449316978455, at3_loss: 5.0986992405910314e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.8771558403968811, at3_loss: 4.782285678572862e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.9766741991043091, at3_loss: 9.324485628070533e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973177909851, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.7500906586647034, at3_loss: 1.011829509067752e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.7300272583961487, at3_loss: 6.55586696041155e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.7404944896697998, at3_loss: 5.487277299209836e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.7767195105552673, at3_loss: 4.606037773413618e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979734420776, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.7913357019424438, at3_loss: 6.970812815865202e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999976754188538, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.7037253975868225, at3_loss: 7.388534228880417e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999969601631165, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.9452860355377197, at3_loss: 1.0803857808383555e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.8156574368476868, at3_loss: 1.2828627049543684e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999996542930603, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.7362479567527771, at3_loss: 1.0628997682005092e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.6987634897232056, at3_loss: 6.901423876826129e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999967813491821, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.7831100225448608, at3_loss: 7.15122405736679e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
cls_loss: 0.7681963443756104, at3_loss: 7.706335569679368e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.7178336381912231, at3_loss: 7.735478924075778e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.7027186155319214, at3_loss: 7.158162951270697e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.6812534332275391, at3_loss: 9.338363415878348e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999960064888, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.7182927131652832, at3_loss: 9.414691248821327e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999975562095642, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.6679320335388184, at3_loss: 8.83182416089312e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999957084655762, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.7993437647819519, at3_loss: 1.7223722448278522e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.666975736618042, at3_loss: 2.8282931552325863e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974966049194, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.586493730545044, at3_loss: 8.815170815523743e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.6739590764045715, at3_loss: 7.897849041427207e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971985816956, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.7406684756278992, at3_loss: 7.290001935444934e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.5955246090888977, at3_loss: 4.754530102957233e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.4694221317768097, at3_loss: 7.25669524470618e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.5639409422874451, at3_loss: 7.43988204376933e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999969005584717, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.6344618797302246, at3_loss: 9.173217740965356e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.5683199763298035, at3_loss: 7.127631818093505e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.7301982641220093, at3_loss: 8.630596237679811e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973773956299, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.56407231092453, at3_loss: 1.2007062011321068e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973773956299, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.5995598435401917, at3_loss: 8.987255384340642e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.5535295605659485, at3_loss: 6.147859998861804e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999975562095642, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.6932610869407654, at3_loss: 8.480716129355415e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971985816956, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.5391838550567627, at3_loss: 1.0320910792671611e-10
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.5097840428352356, at3_loss: 6.7390537594747e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.4527275264263153, at3_loss: 6.544764730165298e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973177909851, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.521675169467926, at3_loss: 7.988054662178001e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.5122849941253662, at3_loss: 4.4603210014315664e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.5719248056411743, at3_loss: 6.415701303552623e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979734420776, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.48466941714286804, at3_loss: 5.77871084317394e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.5379078984260559, at3_loss: 4.3673398231192095e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.5394306778907776, at3_loss: 2.5146551507759796e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974370002747, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.40370941162109375, at3_loss: 5.169475958410885e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.518293023109436, at3_loss: 3.430589146091734e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.46965381503105164, at3_loss: 4.0897840669629204e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.4931861162185669, at3_loss: 4.023170685485411e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 0.9999999403953552
cls_loss: 0.4785139560699463, at3_loss: 3.60961260881254e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.46676620841026306, at3_loss: 5.1736392947532295e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999976754188538, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.40494203567504883, at3_loss: 4.941880238362728e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.5000215172767639, at3_loss: 2.9878877150224525e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.48012110590934753, at3_loss: 2.9656832545299494e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.5447137355804443, at3_loss: 4.686528942698942e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.47011977434158325, at3_loss: 5.079270337660091e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.45673537254333496, at3_loss: 5.956346527113965e-11
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.48950302600860596, at3_loss: 4.470498277120427e-11
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 3.6995996832847595,
 'clean_test_loss_avg_over_batch': 1.1757474559726138,
 'epoch': 0,
 'test_acc': 0.6263095238095238,
 'test_asr': 0.07614285714285714,
 'test_ra': 0.6762857142857143,
 'train_acc': 0.8124537037037037,
 'train_epoch_loss_avg_over_batch': 0.5333775412510423}
2025-03-19:03:11:14 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 3.6995996832847595,
 'clean_test_loss_avg_over_batch': 1.1757474559726138,
 'epoch': 0,
 'test_acc': 0.6263095238095238,
 'test_asr': 0.07614285714285714,
 'test_ra': 0.6762857142857143,
 'train_acc': 0.8124537037037037,
 'train_epoch_loss_avg_over_batch': 0.5333775412510423}
INFO:root:epoch: 1  lr: 0.0100
2025-03-19:03:11:14 [INFO    ] [nad.py:89] epoch: 1  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:11:36 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:11:36 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch1: Loss:32.31422737240791 Training Acc:85.82870370370371(18539/21600)
2025-03-19:03:11:36 [INFO    ] [nad.py:622] Epoch1: Loss:32.31422737240791 Training Acc:85.82870370370371(18539/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.512452244758606, at3_loss: 3.151645611154663e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.39136794209480286, at3_loss: 2.257916076331412e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.4199039041996002, at3_loss: 5.205558206711203e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3718031048774719, at3_loss: 2.295386103412511e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.43018481135368347, at3_loss: 3.443079155118767e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3973069190979004, at3_loss: 1.9040324872321435e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.39353370666503906, at3_loss: 2.20795604022328e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
cls_loss: 0.4932480752468109, at3_loss: 2.518818487118324e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.37519943714141846, at3_loss: 2.5243696022414497e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.3695123493671417, at3_loss: 1.7305601396344628e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.37688690423965454, at3_loss: 3.5513259000197195e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.4800731837749481, at3_loss: 3.0003777240494856e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3743554651737213, at3_loss: 4.779510121011299e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999955892562866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.41972818970680237, at3_loss: 8.144873664406305e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.3549562394618988, at3_loss: 3.22519788653608e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.39774027466773987, at3_loss: 2.3092638912203256e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973177909851, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.38175466656684875, at3_loss: 4.18554080283684e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.4365365207195282, at3_loss: 2.808864252301646e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3972673714160919, at3_loss: 1.6431300764452317e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.49419182538986206, at3_loss: 3.581857033196911e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999976754188538, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3577897548675537, at3_loss: 3.9940273310890007e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999975562095642, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.35778433084487915, at3_loss: 4.5047299224165727e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.403146356344223, at3_loss: 1.8568480086855743e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
cls_loss: 0.3646301329135895, at3_loss: 5.735689700969715e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.42541956901550293, at3_loss: 3.048949981376836e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.37593239545822144, at3_loss: 2.8699265186560297e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3907594084739685, at3_loss: 1.8637869025894815e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.4174470603466034, at3_loss: 1.7319479184152442e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999970197677612, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3077772855758667, at3_loss: 5.7995275248856615e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
cls_loss: 0.3531709313392639, at3_loss: 1.459943277382081e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.36415430903434753, at3_loss: 1.1463052729254741e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.4620670974254608, at3_loss: 2.1399548799649892e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.3682440221309662, at3_loss: 2.506328478091291e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
cls_loss: 0.3450075685977936, at3_loss: 1.3863910020006642e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.38784337043762207, at3_loss: 1.9220736113823023e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.47771933674812317, at3_loss: 4.2243986086987206e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.35492464900016785, at3_loss: 1.6667223157185163e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.4154272973537445, at3_loss: 1.4405143744511406e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.36058104038238525, at3_loss: 1.251776460264864e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.38847431540489197, at3_loss: 2.656208586415687e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.3556841015815735, at3_loss: 2.615963001773025e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974370002747, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.41047313809394836, at3_loss: 2.7922109069322687e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.36352360248565674, at3_loss: 2.285671651947041e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.43151506781578064, at3_loss: 2.0608514894604468e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.43579626083374023, at3_loss: 1.647293412787576e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.4109877347946167, at3_loss: 2.4202861936828413e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.38817939162254333, at3_loss: 1.559863349598345e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.43623772263526917, at3_loss: 2.1788126858268697e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.33021727204322815, at3_loss: 1.6528445279107018e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.48424267768859863, at3_loss: 2.466082893448629e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.4029432535171509, at3_loss: 2.6381674622655282e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.3273215591907501, at3_loss: 1.2961853812498703e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
cls_loss: 0.38995054364204407, at3_loss: 2.157996004115148e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.4489034116268158, at3_loss: 2.3578361485476762e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3650014102458954, at3_loss: 1.454392162258955e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3768732249736786, at3_loss: 1.7694179454963432e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3547530770301819, at3_loss: 1.9623191960249642e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.2770446538925171, at3_loss: 1.3003487175922146e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.29596373438835144, at3_loss: 1.0672018824209317e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.32291746139526367, at3_loss: 1.6209256159527285e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.32049012184143066, at3_loss: 1.4141465776162931e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3618353009223938, at3_loss: 3.709532681028804e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.3190268278121948, at3_loss: 1.797173521111972e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.4000585377216339, at3_loss: 1.8970935933282362e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.28821370005607605, at3_loss: 1.8568480086855743e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.46021631360054016, at3_loss: 1.6958656701149266e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.33025795221328735, at3_loss: 1.4030443473700416e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.4010101854801178, at3_loss: 1.687538997430238e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.33159369230270386, at3_loss: 1.9970136655445003e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.3322385549545288, at3_loss: 1.1185496973098452e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.35205236077308655, at3_loss: 3.437528039995641e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.39921337366104126, at3_loss: 2.307876112439544e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2853561043739319, at3_loss: 8.68749516769185e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.41140350699424744, at3_loss: 2.123301534595612e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.40220144391059875, at3_loss: 1.5737411374061594e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.3806660771369934, at3_loss: 2.6256774532384952e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.2668776512145996, at3_loss: 1.8957058145474548e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3441692292690277, at3_loss: 1.2059797604990763e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.3202340006828308, at3_loss: 1.2587153541687712e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2912789583206177, at3_loss: 1.0380585280245214e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.34410756826400757, at3_loss: 1.534883331544279e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.4114410877227783, at3_loss: 1.7694179454963432e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.36049553751945496, at3_loss: 1.2240208846492351e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3205806314945221, at3_loss: 1.1934897514720433e-11
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3243759870529175, at3_loss: 1.1065223390338552e-11
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 3.6878053971699307,
 'clean_test_loss_avg_over_batch': 1.250310348741936,
 'epoch': 0,
 'test_acc': 0.6308333333333334,
 'test_asr': 0.18271428571428572,
 'test_ra': 0.6354285714285715,
 'train_acc': 0.858287037037037,
 'train_epoch_loss_avg_over_batch': 0.3801673808518578}
2025-03-19:03:11:53 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 3.6878053971699307,
 'clean_test_loss_avg_over_batch': 1.250310348741936,
 'epoch': 0,
 'test_acc': 0.6308333333333334,
 'test_asr': 0.18271428571428572,
 'test_ra': 0.6354285714285715,
 'train_acc': 0.858287037037037,
 'train_epoch_loss_avg_over_batch': 0.3801673808518578}
INFO:root:epoch: 2  lr: 0.0100
2025-03-19:03:11:54 [INFO    ] [nad.py:89] epoch: 2  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:12:19 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:12:19 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch2: Loss:29.04354514181614 Training Acc:87.125(18819/21600)
2025-03-19:03:12:19 [INFO    ] [nad.py:622] Epoch2: Loss:29.04354514181614 Training Acc:87.125(18819/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3231537640094757, at3_loss: 1.700029006457271e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3641299903392792, at3_loss: 1.0283440765590512e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3562433421611786, at3_loss: 2.157996004115148e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.32209354639053345, at3_loss: 1.4266365866433262e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
cls_loss: 0.35517051815986633, at3_loss: 9.325873406851315e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999970197677612, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.34399786591529846, at3_loss: 3.898270595215081e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.26349398493766785, at3_loss: 9.603429163007604e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2891402244567871, at3_loss: 8.271161533457416e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.3180207312107086, at3_loss: 1.7416623698807143e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
cls_loss: 0.3149171769618988, at3_loss: 1.9040324872321435e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2759578227996826, at3_loss: 1.0630385460785874e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 0.9999999403953552
cls_loss: 0.3208891451358795, at3_loss: 2.6020852139652106e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.26997682452201843, at3_loss: 9.131584377541913e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999969005584717, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.26985904574394226, at3_loss: 3.5735303605122226e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.40226995944976807, at3_loss: 7.452372052796363e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999969005584717, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.420467346906662, at3_loss: 4.245215290410442e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.42232444882392883, at3_loss: 1.8207657603852567e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.31238529086112976, at3_loss: 1.3614109839465982e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 0.9999999403953552
cls_loss: 0.41146916151046753, at3_loss: 2.5299207173645755e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.3143819272518158, at3_loss: 1.1435297153639112e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999975562095642, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.35801205039024353, at3_loss: 3.6942671144402084e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.30816414952278137, at3_loss: 2.067790383364354e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971985816956, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.35766133666038513, at3_loss: 3.684552662974738e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.2923126816749573, at3_loss: 1.314226505400029e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.3898021876811981, at3_loss: 1.03667074924374e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.28030017018318176, at3_loss: 1.3572476476042539e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.26740145683288574, at3_loss: 1.1532441668293814e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3548361361026764, at3_loss: 1.1102230246251565e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3356330096721649, at3_loss: 1.2975731600306517e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3319346308708191, at3_loss: 1.429412144204889e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.322273313999176, at3_loss: 1.5959455978986625e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3403227925300598, at3_loss: 7.78543896018391e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2669892907142639, at3_loss: 8.520961713998076e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3531343936920166, at3_loss: 2.3925306180672123e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.42414501309394836, at3_loss: 1.2420620087993939e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.220849871635437, at3_loss: 1.0047518372857667e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.4091695249080658, at3_loss: 9.270362255620057e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3120817542076111, at3_loss: 9.58955137519979e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.33156412839889526, at3_loss: 1.3350431871117507e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.29343146085739136, at3_loss: 7.091549569793187e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.392496258020401, at3_loss: 1.0241807402167069e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.28464484214782715, at3_loss: 1.3225531780847177e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.33875659108161926, at3_loss: 1.7319479184152442e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.34897786378860474, at3_loss: 1.083855227790309e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.26110050082206726, at3_loss: 8.548717289613705e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.32457131147384644, at3_loss: 8.923417560424696e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3251715302467346, at3_loss: 1.6237011735142914e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997615814209, max: 1.0
cls_loss: 0.31515613198280334, at3_loss: 9.700573677662305e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3105868399143219, at3_loss: 1.4280243654241076e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
cls_loss: 0.34166544675827026, at3_loss: 1.865174681370263e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.36036813259124756, at3_loss: 1.2434497875801753e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.33228445053100586, at3_loss: 2.1885271372923398e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3592337965965271, at3_loss: 2.8851920852446256e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.2719746530056, at3_loss: 1.594557819117881e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3917114734649658, at3_loss: 2.0969337377607644e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.34868890047073364, at3_loss: 1.0325074129013956e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3076271414756775, at3_loss: 1.7069679003611782e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.23480050265789032, at3_loss: 1.099120794378905e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
cls_loss: 0.37852543592453003, at3_loss: 1.8457457784393227e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.4107988178730011, at3_loss: 1.2240208846492351e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.4065856635570526, at3_loss: 1.0047518372857667e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3345787227153778, at3_loss: 1.2198575483068907e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
cls_loss: 0.332348108291626, at3_loss: 1.790234627208065e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.35106584429740906, at3_loss: 1.1018963519404679e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3536602854728699, at3_loss: 1.7139067942650854e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.40194442868232727, at3_loss: 1.529332216421153e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
cls_loss: 0.31488606333732605, at3_loss: 1.6861512186494565e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.3711998164653778, at3_loss: 2.6798008256889716e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.24673891067504883, at3_loss: 1.04638520070921e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.39597374200820923, at3_loss: 1.5085155347094314e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
cls_loss: 0.33403486013412476, at3_loss: 1.9720336474904343e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.32568445801734924, at3_loss: 1.1837753000065732e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3495555520057678, at3_loss: 1.6403545188836688e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.34893620014190674, at3_loss: 1.5210055437364645e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3903386890888214, at3_loss: 1.3336554083309693e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.40586137771606445, at3_loss: 1.541822225448186e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.3881080150604248, at3_loss: 1.2850831510036187e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.40702399611473083, at3_loss: 1.8263168755083825e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.3612111210823059, at3_loss: 9.922618282587337e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.36779049038887024, at3_loss: 2.525757381022231e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.41651129722595215, at3_loss: 1.4863110742169283e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.4009295105934143, at3_loss: 1.715294573045867e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.38324567675590515, at3_loss: 1.5737411374061594e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.30293792486190796, at3_loss: 9.520162436160717e-12
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999997615814209, max: 1.0
cls_loss: 0.4928922951221466, at3_loss: 7.623532014000567e-12
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 4.882937908172607,
 'clean_test_loss_avg_over_batch': 1.3582838015122847,
 'epoch': 0,
 'test_acc': 0.6155952380952381,
 'test_asr': 0.046857142857142854,
 'test_ra': 0.688,
 'train_acc': 0.87125,
 'train_epoch_loss_avg_over_batch': 0.3416887663743075}
2025-03-19:03:12:37 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 4.882937908172607,
 'clean_test_loss_avg_over_batch': 1.3582838015122847,
 'epoch': 0,
 'test_acc': 0.6155952380952381,
 'test_asr': 0.046857142857142854,
 'test_ra': 0.688,
 'train_acc': 0.87125,
 'train_epoch_loss_avg_over_batch': 0.3416887663743075}
INFO:root:epoch: 3  lr: 0.0100
2025-03-19:03:12:37 [INFO    ] [nad.py:89] epoch: 3  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:13:01 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:13:01 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch3: Loss:27.244581952691078 Training Acc:87.69444444444444(18942/21600)
2025-03-19:03:13:01 [INFO    ] [nad.py:622] Epoch3: Loss:27.244581952691078 Training Acc:87.69444444444444(18942/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.25216639041900635, at3_loss: 1.0949574580365606e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
cls_loss: 0.4483967423439026, at3_loss: 1.9206858326015208e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.42133861780166626, at3_loss: 1.5501488981328748e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.37634822726249695, at3_loss: 2.746414207166481e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.3516899347305298, at3_loss: 2.6145752229922437e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.37784913182258606, at3_loss: 1.1435297153639112e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3229916989803314, at3_loss: 2.6076363290883364e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.351370632648468, at3_loss: 1.787459069646502e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3252551257610321, at3_loss: 3.5416114485542494e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.35468485951423645, at3_loss: 1.4557799410397365e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3135424554347992, at3_loss: 2.117750419472486e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3649818003177643, at3_loss: 1.0630385460785874e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
cls_loss: 0.3694669008255005, at3_loss: 1.725009024511337e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2853454351425171, at3_loss: 1.1129985821867194e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.33896687626838684, at3_loss: 1.8512968935624485e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3488695025444031, at3_loss: 1.1046719095020308e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3016434609889984, at3_loss: 1.9748092050519972e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3129759132862091, at3_loss: 1.0047518372857667e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.35916006565093994, at3_loss: 1.3974932322469158e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.4603837728500366, at3_loss: 1.1282641487753153e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.29634976387023926, at3_loss: 1.0547118733938987e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
cls_loss: 0.41757938265800476, at3_loss: 2.796374243274613e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.3040452003479004, at3_loss: 1.0477729794899915e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.3101211190223694, at3_loss: 1.0283440765590512e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3381882309913635, at3_loss: 1.0630385460785874e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3314442038536072, at3_loss: 1.7361112547575885e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2881878614425659, at3_loss: 2.058075931898884e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 0.9999999403953552
cls_loss: 0.3093266785144806, at3_loss: 1.0394463068053028e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.31392091512680054, at3_loss: 1.3239409568654992e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.3361393213272095, at3_loss: 8.784639682346551e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974966049194, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.34288081526756287, at3_loss: 2.3397950243975174e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999975562095642, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2772873640060425, at3_loss: 3.083644450896372e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2845899164676666, at3_loss: 9.769962616701378e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.33238667249679565, at3_loss: 1.765254609153999e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.30648499727249146, at3_loss: 1.9914625504213745e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.30523109436035156, at3_loss: 8.81239525796218e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2566849887371063, at3_loss: 1.1921019726912618e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3422093093395233, at3_loss: 1.3752887717544127e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
cls_loss: 0.3067588210105896, at3_loss: 3.767819389821625e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.38482996821403503, at3_loss: 1.214306433183765e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.3881490230560303, at3_loss: 1.840194663316197e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971389770508, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.3201921582221985, at3_loss: 4.246603069191224e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.26399001479148865, at3_loss: 8.81239525796218e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2839626669883728, at3_loss: 8.770761894538737e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3650146722793579, at3_loss: 1.9942381079829374e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3047904372215271, at3_loss: 1.2378986724570495e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3126142919063568, at3_loss: 6.7584826624056404e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3178403675556183, at3_loss: 1.6792123247455493e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.27967578172683716, at3_loss: 1.0644263248593688e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
cls_loss: 0.24810758233070374, at3_loss: 8.978928711655954e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.2807718515396118, at3_loss: 1.759703494030873e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.29772740602493286, at3_loss: 1.304512053934559e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.29477739334106445, at3_loss: 7.757683384568281e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999997615814209, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.29479914903640747, at3_loss: 1.942890293094024e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999964237213135, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.368622362613678, at3_loss: 5.639932965095795e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.3117348849773407, at3_loss: 8.978928711655954e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.35929641127586365, at3_loss: 8.673617379884035e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.2981323301792145, at3_loss: 1.04638520070921e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
cls_loss: 0.3383351266384125, at3_loss: 1.289246487345963e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.37547585368156433, at3_loss: 9.547918011776346e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.2860005497932434, at3_loss: 6.63358257213531e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.29715585708618164, at3_loss: 9.56179579958416e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3197449743747711, at3_loss: 1.0380585280245214e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2469576895236969, at3_loss: 8.40993941153556e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3017961382865906, at3_loss: 9.284240043427872e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.422001451253891, at3_loss: 9.423017921506016e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.21965330839157104, at3_loss: 8.465450562766819e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.24791689217090607, at3_loss: 1.2614909117303341e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.3537585437297821, at3_loss: 1.0644263248593688e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
cls_loss: 0.3355940580368042, at3_loss: 1.049160758270773e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.23129767179489136, at3_loss: 1.9095836023552692e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3701173663139343, at3_loss: 1.419697692739419e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.31322571635246277, at3_loss: 1.5279444376403717e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.28897789120674133, at3_loss: 1.1102230246251565e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.27293142676353455, at3_loss: 1.5307199952019346e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.30957967042922974, at3_loss: 8.770761894538737e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2851865291595459, at3_loss: 8.28503932126523e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.23217596113681793, at3_loss: 7.618905506490137e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.27804794907569885, at3_loss: 1.1934897514720433e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.32303982973098755, at3_loss: 6.772360450213455e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.3383794128894806, at3_loss: 2.167710455580618e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.238967165350914, at3_loss: 6.938893903907228e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.335071325302124, at3_loss: 1.799949078673535e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.35664108395576477, at3_loss: 1.6778245459647678e-11
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 0.9999999403953552
cls_loss: 0.31427955627441406, at3_loss: 5.403085531069385e-12
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 5.2295306239809305,
 'clean_test_loss_avg_over_batch': 1.5095880465074019,
 'epoch': 0,
 'test_acc': 0.601547619047619,
 'test_asr': 0.02557142857142857,
 'test_ra': 0.6832857142857143,
 'train_acc': 0.8769444444444444,
 'train_epoch_loss_avg_over_batch': 0.3205244935610715}
2025-03-19:03:13:21 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 5.2295306239809305,
 'clean_test_loss_avg_over_batch': 1.5095880465074019,
 'epoch': 0,
 'test_acc': 0.601547619047619,
 'test_asr': 0.02557142857142857,
 'test_ra': 0.6832857142857143,
 'train_acc': 0.8769444444444444,
 'train_epoch_loss_avg_over_batch': 0.3205244935610715}
INFO:root:epoch: 4  lr: 0.0100
2025-03-19:03:13:22 [INFO    ] [nad.py:89] epoch: 4  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:13:47 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:13:47 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch4: Loss:24.467141941189766 Training Acc:89.12037037037037(19250/21600)
2025-03-19:03:13:47 [INFO    ] [nad.py:622] Epoch4: Loss:24.467141941189766 Training Acc:89.12037037037037(19250/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
cls_loss: 0.27701079845428467, at3_loss: 1.3031242751537775e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.26841962337493896, at3_loss: 1.4488410471358293e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.25812724232673645, at3_loss: 8.118505867571457e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3409886360168457, at3_loss: 8.784639682346551e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.3254428505897522, at3_loss: 8.798517470154366e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2830298840999603, at3_loss: 9.020562075079397e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 1.0
cls_loss: 0.28826236724853516, at3_loss: 2.3037127760971998e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.28719016909599304, at3_loss: 1.534883331544279e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.25446051359176636, at3_loss: 1.905420266012925e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.35296136140823364, at3_loss: 8.978928711655954e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999962449073792, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.24298657476902008, at3_loss: 4.911349105185536e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
cls_loss: 0.33673056960105896, at3_loss: 2.4077961846558082e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.2684156000614166, at3_loss: 8.104628079763643e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.3175598382949829, at3_loss: 7.521760991835436e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999997615814209, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.35394543409347534, at3_loss: 2.9601321394068236e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.31511858105659485, at3_loss: 1.8207657603852567e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.3604724109172821, at3_loss: 9.825473767932635e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973773956299, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.3216726779937744, at3_loss: 2.824129818890242e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.27474135160446167, at3_loss: 9.756084828893563e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2603747844696045, at3_loss: 1.4238610290817633e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.24204078316688538, at3_loss: 1.4072076837123859e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.3547254204750061, at3_loss: 1.607047828144914e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.3116377294063568, at3_loss: 2.4397150966137815e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3041549026966095, at3_loss: 7.452372052796363e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3038826286792755, at3_loss: 7.313594174718219e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.28703781962394714, at3_loss: 1.2975731600306517e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.35358160734176636, at3_loss: 8.493206138382448e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.28165513277053833, at3_loss: 2.7797208979052357e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.3721606433391571, at3_loss: 8.576472865229334e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.32242143154144287, at3_loss: 8.382183835919932e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.31465378403663635, at3_loss: 1.3433698597964394e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.24246026575565338, at3_loss: 1.239286451237831e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.2704545259475708, at3_loss: 1.5612511283791264e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
cls_loss: 0.27509158849716187, at3_loss: 1.6306400674181987e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2546190023422241, at3_loss: 7.355227538141662e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.22403253614902496, at3_loss: 1.1268763699945339e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.28860190510749817, at3_loss: 1.0824674490095276e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.25115370750427246, at3_loss: 1.1393663790215669e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.22934085130691528, at3_loss: 1.892930256985892e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.2975733280181885, at3_loss: 1.0325074129013956e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.2540566921234131, at3_loss: 6.9111383282915995e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.30080947279930115, at3_loss: 1.2254086634300165e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2790910303592682, at3_loss: 1.226796442210798e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.38928040862083435, at3_loss: 9.922618282587337e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.27986887097358704, at3_loss: 1.008915173628111e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.29416385293006897, at3_loss: 1.9109713811360507e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.31212857365608215, at3_loss: 6.980527267330672e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2952483594417572, at3_loss: 8.576472865229334e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.283200204372406, at3_loss: 5.6343818499726694e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971389770508, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2677987515926361, at3_loss: 3.375077994860476e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999961853027344, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.22618989646434784, at3_loss: 4.969635813978357e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.2562309801578522, at3_loss: 7.93809462606987e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 1.0
cls_loss: 0.21317178010940552, at3_loss: 1.684763439868675e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.312165230512619, at3_loss: 6.189493362285248e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3056487441062927, at3_loss: 8.548717289613705e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.33461540937423706, at3_loss: 9.978129433818594e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2874525785446167, at3_loss: 1.354472090042691e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.26118722558021545, at3_loss: 9.298117831235686e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 1.0
cls_loss: 0.36503687500953674, at3_loss: 1.587618925213974e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
cls_loss: 0.25310030579566956, at3_loss: 1.2476131239225197e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.24781757593154907, at3_loss: 1.4280243654241076e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3020971119403839, at3_loss: 8.18789480661053e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.3269415497779846, at3_loss: 1.3100631690576847e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999997615814209, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3365050256252289, at3_loss: 1.9623191960249642e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.32015305757522583, at3_loss: 8.631984016460592e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.2380295693874359, at3_loss: 6.175615574477433e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.2846320569515228, at3_loss: 9.672818102046676e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2374226450920105, at3_loss: 6.952771691715043e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.30630263686180115, at3_loss: 1.1310397063368782e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.2268594205379486, at3_loss: 7.951972413877684e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.31828081607818604, at3_loss: 8.229528170033973e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.3241431415081024, at3_loss: 8.007483565108942e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.2676760256290436, at3_loss: 7.882583474838611e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3066113293170929, at3_loss: 1.2934098236883074e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.21537135541439056, at3_loss: 7.16093850883226e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.30150818824768066, at3_loss: 6.13398221105399e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.20605163276195526, at3_loss: 9.284240043427872e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.24850961565971375, at3_loss: 1.1546319456101628e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.2434263527393341, at3_loss: 1.1823875212257917e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2659013569355011, at3_loss: 7.702172233337023e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.24356010556221008, at3_loss: 1.2947976024690888e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.31178075075149536, at3_loss: 1.04638520070921e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.30728623270988464, at3_loss: 1.186550857568136e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.29418930411338806, at3_loss: 7.188694084447889e-12
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.24644897878170013, at3_loss: 3.093821726585233e-11
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 4.785207067217145,
 'clean_test_loss_avg_over_batch': 1.3444333365469268,
 'epoch': 0,
 'test_acc': 0.6273809523809524,
 'test_asr': 0.08328571428571428,
 'test_ra': 0.6655714285714286,
 'train_acc': 0.8912037037037037,
 'train_epoch_loss_avg_over_batch': 0.2878487287198796}
2025-03-19:03:14:03 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 4.785207067217145,
 'clean_test_loss_avg_over_batch': 1.3444333365469268,
 'epoch': 0,
 'test_acc': 0.6273809523809524,
 'test_asr': 0.08328571428571428,
 'test_ra': 0.6655714285714286,
 'train_acc': 0.8912037037037037,
 'train_epoch_loss_avg_over_batch': 0.2878487287198796}
INFO:root:epoch: 5  lr: 0.0100
2025-03-19:03:14:03 [INFO    ] [nad.py:89] epoch: 5  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:14:29 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:14:29 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch5: Loss:22.338485792279243 Training Acc:89.99074074074075(19438/21600)
2025-03-19:03:14:29 [INFO    ] [nad.py:622] Epoch5: Loss:22.338485792279243 Training Acc:89.99074074074075(19438/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.275691956281662, at3_loss: 8.992806499463768e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.24735978245735168, at3_loss: 2.628453010800058e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.34407395124435425, at3_loss: 7.979727989493313e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2225102186203003, at3_loss: 1.2725931419765857e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.29290571808815, at3_loss: 2.1871393585115584e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.23413103818893433, at3_loss: 1.058875209736243e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3119746148586273, at3_loss: 6.3976601794024646e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2885357737541199, at3_loss: 6.2450045135165055e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.32005608081817627, at3_loss: 7.0637939941775585e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.22516487538814545, at3_loss: 1.9220736113823023e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.26247698068618774, at3_loss: 8.715250743307479e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3387238085269928, at3_loss: 2.2565282975506307e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
cls_loss: 0.26377883553504944, at3_loss: 9.173217740965356e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.20416830480098724, at3_loss: 1.0755285551056204e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.22241976857185364, at3_loss: 5.981326545168031e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.20592382550239563, at3_loss: 7.188694084447889e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.2801768481731415, at3_loss: 6.661338147750939e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2308119833469391, at3_loss: 6.952771691715043e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.24634668231010437, at3_loss: 1.7166823518266483e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2507113218307495, at3_loss: 8.743006318923108e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2549194395542145, at3_loss: 9.58955137519979e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.23580507934093475, at3_loss: 8.021361352916756e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2390289604663849, at3_loss: 8.40993941153556e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2612861096858978, at3_loss: 1.952604744559494e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.33657366037368774, at3_loss: 9.506284648352903e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973177909851, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.33464327454566956, at3_loss: 2.391142839286431e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.210775688290596, at3_loss: 8.951173136040325e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
cls_loss: 0.2638024687767029, at3_loss: 2.4369395390522186e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.29979652166366577, at3_loss: 1.2545520178264269e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.227378711104393, at3_loss: 5.023759186428833e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2571779191493988, at3_loss: 9.56179579958416e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.30729660391807556, at3_loss: 8.729128531115293e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2866593599319458, at3_loss: 7.008282842946301e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.25397008657455444, at3_loss: 9.076073226310655e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.266135573387146, at3_loss: 9.089951014118469e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.26491451263427734, at3_loss: 1.1379786002407855e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.25644952058792114, at3_loss: 7.577272143066693e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.2716480493545532, at3_loss: 2.002564780667626e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.26711806654930115, at3_loss: 7.507883204027621e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.20667114853858948, at3_loss: 7.494005416219807e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.2237609624862671, at3_loss: 1.0547118733938987e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.23042292892932892, at3_loss: 5.495603971894525e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.25297123193740845, at3_loss: 1.2531642390456454e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.25024059414863586, at3_loss: 4.690692279041286e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.24702171981334686, at3_loss: 7.202571872255703e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.2657182216644287, at3_loss: 2.1593837828959295e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.33627697825431824, at3_loss: 6.25888230132432e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
cls_loss: 0.30812597274780273, at3_loss: 8.0629947163402e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.26289770007133484, at3_loss: 7.840950111415168e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.18920712172985077, at3_loss: 5.620504062164855e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2167349010705948, at3_loss: 1.0810796702287462e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2559911608695984, at3_loss: 4.676814491233472e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2997649013996124, at3_loss: 8.03523914072457e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.3020269572734833, at3_loss: 1.2087553180606392e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.32185518741607666, at3_loss: 4.704570066849101e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.25134149193763733, at3_loss: 9.492406860545088e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.29221171140670776, at3_loss: 4.760081218080359e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
cls_loss: 0.2832275927066803, at3_loss: 6.897260540483785e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.30488818883895874, at3_loss: 9.423017921506016e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
cls_loss: 0.24696163833141327, at3_loss: 7.646661082105766e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.28102028369903564, at3_loss: 7.618905506490137e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
cls_loss: 0.23843027651309967, at3_loss: 1.6611712005953905e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.24580557644367218, at3_loss: 8.076872504148014e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.32111793756484985, at3_loss: 6.5503158452884236e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.27899572253227234, at3_loss: 5.23192600354605e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.31910985708236694, at3_loss: 4.898859096158503e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.25525835156440735, at3_loss: 6.286637876939949e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.21847954392433167, at3_loss: 6.217248937900877e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.23889228701591492, at3_loss: 9.950373858202965e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.23087528347969055, at3_loss: 4.9960036108132044e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2841379940509796, at3_loss: 6.328271240363392e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.22396783530712128, at3_loss: 5.162537064506978e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.26809173822402954, at3_loss: 1.2365108936762681e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.23838308453559875, at3_loss: 1.2129186544029835e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.24921759963035583, at3_loss: 9.658940314238862e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.2610139846801758, at3_loss: 6.25888230132432e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2141587883234024, at3_loss: 1.3031242751537775e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.26379868388175964, at3_loss: 9.867107131356079e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999997079372406, max: 1.0
cls_loss: 0.24748466908931732, at3_loss: 4.40619762898109e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.3269106149673462, at3_loss: 1.441902153231922e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.27456721663475037, at3_loss: 1.1102230246251565e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.29699450731277466, at3_loss: 6.369904603786836e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2933095097541809, at3_loss: 7.16093850883226e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974966049194, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.20274509489536285, at3_loss: 2.1344037648418634e-11
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.15407906472682953, at3_loss: 1.0954201087876037e-11
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 4.639819758278983,
 'clean_test_loss_avg_over_batch': 1.4401553724751328,
 'epoch': 0,
 'test_acc': 0.6278571428571429,
 'test_asr': 0.12242857142857143,
 'test_ra': 0.6492857142857142,
 'train_acc': 0.8999074074074074,
 'train_epoch_loss_avg_over_batch': 0.2628057152032852}
2025-03-19:03:14:45 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 4.639819758278983,
 'clean_test_loss_avg_over_batch': 1.4401553724751328,
 'epoch': 0,
 'test_acc': 0.6278571428571429,
 'test_asr': 0.12242857142857143,
 'test_ra': 0.6492857142857142,
 'train_acc': 0.8999074074074074,
 'train_epoch_loss_avg_over_batch': 0.2628057152032852}
INFO:root:epoch: 6  lr: 0.0100
2025-03-19:03:14:46 [INFO    ] [nad.py:89] epoch: 6  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:15:10 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:15:10 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch6: Loss:21.35820345580578 Training Acc:90.21296296296296(19486/21600)
2025-03-19:03:15:10 [INFO    ] [nad.py:622] Epoch6: Loss:21.35820345580578 Training Acc:90.21296296296296(19486/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.27021244168281555, at3_loss: 6.2727600891321345e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.20227061212062836, at3_loss: 6.591949208711867e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 0.9999999403953552
cls_loss: 0.22388441860675812, at3_loss: 1.5210055437364645e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.2932535707950592, at3_loss: 6.161737786669619e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.20475097000598907, at3_loss: 7.577272143066693e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.18958114087581635, at3_loss: 5.23192600354605e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.28042134642601013, at3_loss: 1.6764367671839864e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.19234700500965118, at3_loss: 1.0658141036401503e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2515331208705902, at3_loss: 7.591149930874508e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.2700967490673065, at3_loss: 1.6028844918025698e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.2374303787946701, at3_loss: 5.2735593669694936e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999961256980896, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.22296127676963806, at3_loss: 6.286637876939949e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.21676549315452576, at3_loss: 8.590350653037149e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2251443713903427, at3_loss: 6.078471059822732e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
cls_loss: 0.18146640062332153, at3_loss: 8.382183835919932e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 1.0
cls_loss: 0.24797984957695007, at3_loss: 1.441902153231922e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3095904588699341, at3_loss: 6.342149028171207e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2406531572341919, at3_loss: 6.925016116099414e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.27076947689056396, at3_loss: 7.924216838262055e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.2081645131111145, at3_loss: 6.189493362285248e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.24633488059043884, at3_loss: 8.618106228652778e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3038557469844818, at3_loss: 9.117706589734098e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980926513672, max: 1.0
cls_loss: 0.38055333495140076, at3_loss: 2.1399548799649892e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2764208912849426, at3_loss: 8.354428260304303e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.23791150748729706, at3_loss: 5.925815393936773e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.24007295072078705, at3_loss: 5.4817261840867104e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.24212516844272614, at3_loss: 8.756884106730922e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.23685863614082336, at3_loss: 8.021361352916756e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.26270347833633423, at3_loss: 6.786238238021269e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.26391100883483887, at3_loss: 7.577272143066693e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.21517916023731232, at3_loss: 2.5993096564036478e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.23133812844753265, at3_loss: 1.4960255256823984e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.3220977187156677, at3_loss: 6.161737786669619e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.27268579602241516, at3_loss: 8.298917109073045e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.22691546380519867, at3_loss: 1.7513768213461844e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.22451461851596832, at3_loss: 6.591949208711867e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.28254085779190063, at3_loss: 5.981326545168031e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.2155430018901825, at3_loss: 6.6058269965196814e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2613483667373657, at3_loss: 8.701372955499664e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.22879229485988617, at3_loss: 7.327471962526033e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.2186320573091507, at3_loss: 8.854028621385623e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.19702449440956116, at3_loss: 7.507883204027621e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.24426865577697754, at3_loss: 5.10702591327572e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.26558661460876465, at3_loss: 1.2101430968414206e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.324916273355484, at3_loss: 1.1657341758564144e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999997079372406, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.24055424332618713, at3_loss: 3.490263633665336e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.22214341163635254, at3_loss: 8.021361352916756e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.30993443727493286, at3_loss: 7.799316747991725e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.21525715291500092, at3_loss: 9.33975119465913e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.25520747900009155, at3_loss: 8.340550472496489e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.3107064962387085, at3_loss: 1.8707257964933888e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 0.9999999403953552
cls_loss: 0.26708486676216125, at3_loss: 2.070565940925917e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.2910536527633667, at3_loss: 7.008282842946301e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.32603541016578674, at3_loss: 9.43689570931383e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999972581863403, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.32597410678863525, at3_loss: 3.447242491461111e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2438492774963379, at3_loss: 1.827704654289164e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980926513672, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.2336527407169342, at3_loss: 1.4488410471358293e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.23114241659641266, at3_loss: 7.147060721024445e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.27144473791122437, at3_loss: 1.0144662887512368e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.21506255865097046, at3_loss: 4.5102810375396984e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.25208237767219543, at3_loss: 6.00908212078366e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.26706528663635254, at3_loss: 8.118505867571457e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.23710468411445618, at3_loss: 8.659739592076221e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.22891604900360107, at3_loss: 9.534040223968532e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.23650798201560974, at3_loss: 1.4280243654241076e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.17099732160568237, at3_loss: 7.396860901565105e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.2523512840270996, at3_loss: 7.424616477180734e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.28588587045669556, at3_loss: 5.842548667089886e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.24506019055843353, at3_loss: 6.480926906249351e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.24489718675613403, at3_loss: 6.6058269965196814e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2505606710910797, at3_loss: 6.716849298982197e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.22349226474761963, at3_loss: 6.661338147750939e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979734420776, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
cls_loss: 0.26870667934417725, at3_loss: 2.54379850517239e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.21112582087516785, at3_loss: 6.952771691715043e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.23417286574840546, at3_loss: 7.133182933216631e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.20558181405067444, at3_loss: 5.9674487573602164e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.27420568466186523, at3_loss: 6.7584826624056404e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.22118458151817322, at3_loss: 5.662137425588298e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971985816956, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.3083275854587555, at3_loss: 2.8976820942716586e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977350234985, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.29701271653175354, at3_loss: 2.108035968007016e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.22780169546604156, at3_loss: 9.228728892196614e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.3178584575653076, at3_loss: 6.147859998861804e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.2309679388999939, at3_loss: 9.228728892196614e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.18799301981925964, at3_loss: 1.2753686995381486e-11
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999997615814209, max: 1.0
cls_loss: 0.35783258080482483, at3_loss: 3.848773296594166e-12
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 4.772993700844901,
 'clean_test_loss_avg_over_batch': 1.2860995350462017,
 'epoch': 0,
 'test_acc': 0.6466666666666666,
 'test_asr': 0.07414285714285715,
 'test_ra': 0.6948571428571428,
 'train_acc': 0.9021296296296296,
 'train_epoch_loss_avg_over_batch': 0.25127298183300917}
2025-03-19:03:15:26 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 4.772993700844901,
 'clean_test_loss_avg_over_batch': 1.2860995350462017,
 'epoch': 0,
 'test_acc': 0.6466666666666666,
 'test_asr': 0.07414285714285715,
 'test_ra': 0.6948571428571428,
 'train_acc': 0.9021296296296296,
 'train_epoch_loss_avg_over_batch': 0.25127298183300917}
INFO:root:epoch: 7  lr: 0.0100
2025-03-19:03:15:27 [INFO    ] [nad.py:89] epoch: 7  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:15:46 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:15:46 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch7: Loss:20.499050199985504 Training Acc:91.00925925925925(19658/21600)
2025-03-19:03:15:46 [INFO    ] [nad.py:622] Epoch7: Loss:20.499050199985504 Training Acc:91.00925925925925(19658/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.21668994426727295, at3_loss: 9.43689570931383e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.16538357734680176, at3_loss: 1.0533240946131173e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.16906744241714478, at3_loss: 6.328271240363392e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.26086297631263733, at3_loss: 7.355227538141662e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.23499204218387604, at3_loss: 5.6066262743570405e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.23371052742004395, at3_loss: 6.022959908591474e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2658119797706604, at3_loss: 9.089951014118469e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985694885254, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.20816965401172638, at3_loss: 1.2087553180606392e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
cls_loss: 0.2500994801521301, at3_loss: 1.4752088439706768e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.26217326521873474, at3_loss: 7.743805596760467e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.20331008732318878, at3_loss: 5.703770789011742e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980330467224, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.2378862202167511, at3_loss: 1.4058199049316045e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.18542911112308502, at3_loss: 6.2727600891321345e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.1944139450788498, at3_loss: 6.869504964868156e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.2587297260761261, at3_loss: 6.25888230132432e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999970197677612, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.32032451033592224, at3_loss: 3.5513259000197195e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999978542327881, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2134249359369278, at3_loss: 1.634803403760543e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974370002747, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2370203286409378, at3_loss: 3.13915560212763e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.2573392987251282, at3_loss: 9.46465128492946e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982118606567, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2800773084163666, at3_loss: 1.1657341758564144e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2722570598125458, at3_loss: 5.7870375158586285e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.27080482244491577, at3_loss: 5.953570969552402e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.2743641138076782, at3_loss: 5.4817261840867104e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.22955036163330078, at3_loss: 5.537237335317968e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
cls_loss: 0.23201270401477814, at3_loss: 6.855627177060342e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.23570457100868225, at3_loss: 4.8433479449272454e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.2024429887533188, at3_loss: 5.995204332975845e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.385027676820755, at3_loss: 1.0699774399824946e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2230287492275238, at3_loss: 4.829470157119431e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.20204442739486694, at3_loss: 7.632783294297951e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.25484564900398254, at3_loss: 6.2727600891321345e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.21518129110336304, at3_loss: 4.20496970576778e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.27067816257476807, at3_loss: 7.618905506490137e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.22950786352157593, at3_loss: 6.175615574477433e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.27008700370788574, at3_loss: 7.2164496600635175e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.22907212376594543, at3_loss: 5.7870375158586285e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2866532802581787, at3_loss: 5.551115123125783e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.23558251559734344, at3_loss: 6.050715484207103e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.294635146856308, at3_loss: 1.0769163338864018e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2576144337654114, at3_loss: 7.494005416219807e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.20420224964618683, at3_loss: 7.993605777301127e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.19000156223773956, at3_loss: 4.274358644806853e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.300374835729599, at3_loss: 7.965850201685498e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.267316073179245, at3_loss: 5.7592819402429996e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.26416128873825073, at3_loss: 6.13398221105399e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2926708459854126, at3_loss: 6.702971511174383e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.29773449897766113, at3_loss: 6.356026815979021e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.28299152851104736, at3_loss: 5.509481759702339e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2881528437137604, at3_loss: 8.229528170033973e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2615887224674225, at3_loss: 6.9111383282915995e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.24400140345096588, at3_loss: 6.25888230132432e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.27400094270706177, at3_loss: 8.840150833577809e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.1848868727684021, at3_loss: 5.1486592766991635e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.27025169134140015, at3_loss: 6.994405055138486e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.20306846499443054, at3_loss: 6.591949208711867e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.27128520607948303, at3_loss: 7.743805596760467e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.18830084800720215, at3_loss: 6.022959908591474e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
cls_loss: 0.18039266765117645, at3_loss: 1.8943180357666733e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.22034993767738342, at3_loss: 1.2351231148954867e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2688969671726227, at3_loss: 6.7307270867900115e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2394598126411438, at3_loss: 6.800116025829084e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.25402817130088806, at3_loss: 1.1504686092678185e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.29579052329063416, at3_loss: 9.506284648352903e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2428710162639618, at3_loss: 9.853229343548264e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.19480784237384796, at3_loss: 6.189493362285248e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999974966049194, max: 1.0
cls_loss: 0.2419402301311493, at3_loss: 3.1155633628543455e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.18405206501483917, at3_loss: 7.188694084447889e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.1904720664024353, at3_loss: 7.091549569793187e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.21320374310016632, at3_loss: 6.4254157550180935e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.18103757500648499, at3_loss: 1.086630785351872e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
cls_loss: 0.33867132663726807, at3_loss: 8.618106228652778e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 0.9999999403953552
cls_loss: 0.170651376247406, at3_loss: 5.7870375158586285e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2532689571380615, at3_loss: 1.0394463068053028e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.26523709297180176, at3_loss: 5.9396931817445875e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.2574790418148041, at3_loss: 9.520162436160717e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.30174386501312256, at3_loss: 7.202571872255703e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.172006756067276, at3_loss: 1.136590821460004e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.23379789292812347, at3_loss: 8.673617379884035e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.21629412472248077, at3_loss: 7.355227538141662e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
cls_loss: 0.22859136760234833, at3_loss: 1.3281042932078435e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.29008230566978455, at3_loss: 1.1809997424450103e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.20331986248493195, at3_loss: 5.717648576819556e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.18260900676250458, at3_loss: 7.41073868937292e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.16328486800193787, at3_loss: 4.427014310692812e-12
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.303708553314209, at3_loss: 1.1324274851176597e-11
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 4.872410195214408,
 'clean_test_loss_avg_over_batch': 1.272439631548795,
 'epoch': 0,
 'test_acc': 0.6502380952380953,
 'test_asr': 0.08671428571428572,
 'test_ra': 0.6942857142857143,
 'train_acc': 0.9100925925925926,
 'train_epoch_loss_avg_over_batch': 0.2411652964704177}
2025-03-19:03:15:54 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 4.872410195214408,
 'clean_test_loss_avg_over_batch': 1.272439631548795,
 'epoch': 0,
 'test_acc': 0.6502380952380953,
 'test_asr': 0.08671428571428572,
 'test_ra': 0.6942857142857143,
 'train_acc': 0.9100925925925926,
 'train_epoch_loss_avg_over_batch': 0.2411652964704177}
INFO:root:epoch: 8  lr: 0.0100
2025-03-19:03:15:54 [INFO    ] [nad.py:89] epoch: 8  lr: 0.0100
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:16:15 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:16:15 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch8: Loss:20.63798314332962 Training Acc:90.7824074074074(19609/21600)
2025-03-19:03:16:15 [INFO    ] [nad.py:622] Epoch8: Loss:20.63798314332962 Training Acc:90.7824074074074(19609/21600)
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.1774923950433731, at3_loss: 4.565792188770956e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.19837304949760437, at3_loss: 8.604228440844963e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.20291641354560852, at3_loss: 5.162537064506978e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.23727889358997345, at3_loss: 6.689093723366568e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.23545843362808228, at3_loss: 7.0360384185619296e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.27487051486968994, at3_loss: 9.811595980124821e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.17862150073051453, at3_loss: 7.077671781985373e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2570490539073944, at3_loss: 6.328271240363392e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999959468841553, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.1792496144771576, at3_loss: 4.900246874939285e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.29388049244880676, at3_loss: 8.326672684688674e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.20764188468456268, at3_loss: 8.382183835919932e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.26756003499031067, at3_loss: 7.299716386910404e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.2466052770614624, at3_loss: 4.6629367034256575e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.21889813244342804, at3_loss: 6.453171330633722e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.21500973403453827, at3_loss: 4.8433479449272454e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.20082759857177734, at3_loss: 5.592748486549226e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.2286812663078308, at3_loss: 1.111610803405938e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
cls_loss: 0.2280758023262024, at3_loss: 7.993605777301127e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.24783217906951904, at3_loss: 8.368306048112117e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.22678934037685394, at3_loss: 5.204170427930421e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.22405388951301575, at3_loss: 8.021361352916756e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2253657877445221, at3_loss: 1.1907141939104804e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.25278282165527344, at3_loss: 4.010680676458378e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.20486660301685333, at3_loss: 5.204170427930421e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.19191640615463257, at3_loss: 5.662137425588298e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.20953546464443207, at3_loss: 7.0360384185619296e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.1926518827676773, at3_loss: 5.2735593669694936e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999985098838806, max: 0.9999999403953552
cls_loss: 0.3665451407432556, at3_loss: 1.0352829704629585e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.26896896958351135, at3_loss: 7.979727989493313e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2565675675868988, at3_loss: 1.1407541578023483e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999973773956299, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.2067282348871231, at3_loss: 3.322342401190781e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.268645703792572, at3_loss: 7.882583474838611e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.37307578325271606, at3_loss: 6.161737786669619e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2513003349304199, at3_loss: 8.507083926190262e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.2728820741176605, at3_loss: 8.951173136040325e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.19397731125354767, at3_loss: 5.093148125467906e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 1.0
cls_loss: 0.34295836091041565, at3_loss: 1.2490009027033011e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.23568245768547058, at3_loss: 8.382183835919932e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.21472090482711792, at3_loss: 5.551115123125783e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2394411861896515, at3_loss: 6.3976601794024646e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.31288468837738037, at3_loss: 1.2281842209915794e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.2600174844264984, at3_loss: 1.84297022087776e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999984502792358, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.26375332474708557, at3_loss: 1.3961054534661343e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.24694094061851501, at3_loss: 1.0644263248593688e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.22328443825244904, at3_loss: 7.382983113757291e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.23774126172065735, at3_loss: 6.869504964868156e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.263893723487854, at3_loss: 7.688294445529209e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.24564948678016663, at3_loss: 1.0907941216942163e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 1.0
cls_loss: 0.22671130299568176, at3_loss: 1.634803403760543e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.1975429207086563, at3_loss: 6.647460359943125e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.22768534719944, at3_loss: 6.439293542825908e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.23038184642791748, at3_loss: 5.689893001203927e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.18330974876880646, at3_loss: 4.6351811278100286e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.2314157336950302, at3_loss: 8.229528170033973e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979734420776, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999989867210388, max: 1.0
cls_loss: 0.28631430864334106, at3_loss: 2.4452662117369073e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.21942679584026337, at3_loss: 6.217248937900877e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.29363974928855896, at3_loss: 6.980527267330672e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2560955286026001, at3_loss: 7.674416657721395e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
cls_loss: 0.22048473358154297, at3_loss: 7.341349750333848e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2657220959663391, at3_loss: 9.575673587391975e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.20699894428253174, at3_loss: 1.2989609388114332e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.23487308621406555, at3_loss: 6.356026815979021e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.17370326817035675, at3_loss: 5.995204332975845e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2309092879295349, at3_loss: 5.564992910933597e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.20585304498672485, at3_loss: 5.9119376061289586e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2553219497203827, at3_loss: 9.478529072737274e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.26897603273391724, at3_loss: 5.495603971894525e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.27961859107017517, at3_loss: 1.472433286409114e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.22426123917102814, at3_loss: 1.7943979635504093e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.19809558987617493, at3_loss: 6.925016116099414e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 1.0
cls_loss: 0.20606687664985657, at3_loss: 9.33975119465913e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.24578578770160675, at3_loss: 6.467049118441537e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2564861476421356, at3_loss: 7.827072323607354e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
cls_loss: 0.2719506323337555, at3_loss: 6.702971511174383e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.24542203545570374, at3_loss: 8.729128531115293e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.270744651556015, at3_loss: 8.104628079763643e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2490871399641037, at3_loss: 4.093947403305265e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.25177448987960815, at3_loss: 6.356026815979021e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.26606887578964233, at3_loss: 8.784639682346551e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.2362094521522522, at3_loss: 6.675215935558754e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.23861217498779297, at3_loss: 7.743805596760467e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.3701329827308655, at3_loss: 7.174816296640074e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.24047355353832245, at3_loss: 1.0200174038743626e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999982714653015, max: 1.0
cls_loss: 0.2789018452167511, at3_loss: 1.6806001035263307e-11
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 0.9999999403953552
cls_loss: 0.32298150658607483, at3_loss: 2.749652415479087e-11
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 4.737339564732143,
 'clean_test_loss_avg_over_batch': 1.278940782402501,
 'epoch': 0,
 'test_acc': 0.6632142857142858,
 'test_asr': 0.13742857142857143,
 'test_ra': 0.6831428571428572,
 'train_acc': 0.9078240740740741,
 'train_epoch_loss_avg_over_batch': 0.24279980168623083}
2025-03-19:03:16:27 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 4.737339564732143,
 'clean_test_loss_avg_over_batch': 1.278940782402501,
 'epoch': 0,
 'test_acc': 0.6632142857142858,
 'test_asr': 0.13742857142857143,
 'test_ra': 0.6831428571428572,
 'train_acc': 0.9078240740740741,
 'train_epoch_loss_avg_over_batch': 0.24279980168623083}
INFO:root:epoch: 9  lr: 0.0100
2025-03-19:03:16:28 [INFO    ] [nad.py:89] epoch: 9  lr: 0.0100
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.24878056347370148, at3_loss: 5.88418203051333e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.21298234164714813, at3_loss: 9.811595980124821e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.21968898177146912, at3_loss: 5.9674487573602164e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999979138374329, max: 0.9999999403953552
cls_loss: 0.22047993540763855, at3_loss: 2.5826563110342704e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.22827839851379395, at3_loss: 6.536438057480609e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.22840958833694458, at3_loss: 7.591149930874508e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2593126893043518, at3_loss: 8.229528170033973e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.23934143781661987, at3_loss: 8.28503932126523e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.22097866237163544, at3_loss: 5.856426454897701e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.21964402496814728, at3_loss: 7.174816296640074e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.21839933097362518, at3_loss: 7.258083023486961e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.2389720380306244, at3_loss: 6.3976601794024646e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.24532011151313782, at3_loss: 1.5640266859406893e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.14345595240592957, at3_loss: 6.369904603786836e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.18927985429763794, at3_loss: 1.2337353361147052e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 0.9999999403953552
cls_loss: 0.22924701869487762, at3_loss: 7.41073868937292e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
cls_loss: 0.23902010917663574, at3_loss: 1.491862189340054e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.1998879760503769, at3_loss: 6.342149028171207e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.21325956284999847, at3_loss: 7.2442052356791464e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.18752695620059967, at3_loss: 1.2531642390456454e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.26178738474845886, at3_loss: 6.661338147750939e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.21386465430259705, at3_loss: 6.147859998861804e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.23690368235111237, at3_loss: 7.466249840604178e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.22861529886722565, at3_loss: 1.0241807402167069e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.20081129670143127, at3_loss: 5.495603971894525e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.21651916205883026, at3_loss: 6.744604874597826e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.31228721141815186, at3_loss: 8.076872504148014e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.25497889518737793, at3_loss: 8.03523914072457e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.19216109812259674, at3_loss: 6.328271240363392e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.16869260370731354, at3_loss: 6.897260540483785e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2704488933086395, at3_loss: 1.702804564018834e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
cls_loss: 0.2116159200668335, at3_loss: 9.922618282587337e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998927116394, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.16293787956237793, at3_loss: 7.507883204027621e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.2928961515426636, at3_loss: 1.3600232051658168e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.1953527182340622, at3_loss: 6.9111383282915995e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.17109407484531403, at3_loss: 5.689893001203927e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.21215033531188965, at3_loss: 4.08006961549745e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.23905569314956665, at3_loss: 6.3976601794024646e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.20726297795772552, at3_loss: 6.869504964868156e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999997019767761, max: 1.0
cls_loss: 0.22368502616882324, at3_loss: 3.6221026178395732e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.2779722511768341, at3_loss: 8.396061623727746e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.22752881050109863, at3_loss: 3.969047313034935e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983906745911, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.1748826652765274, at3_loss: 1.2698175844150228e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.22967013716697693, at3_loss: 5.079270337660091e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.21844397485256195, at3_loss: 5.592748486549226e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999966621398926, max: 1.0
cls_loss: 0.21298474073410034, at3_loss: 4.6976311729451936e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.22341486811637878, at3_loss: 8.146261443187086e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.21898004412651062, at3_loss: 6.786238238021269e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.1706792116165161, at3_loss: 6.619704784327496e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.235831618309021, at3_loss: 6.494804694057166e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
cls_loss: 0.2075517773628235, at3_loss: 7.271960811294775e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.21240466833114624, at3_loss: 8.798517470154366e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999975562095642, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.28032156825065613, at3_loss: 2.946254351599009e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
cls_loss: 0.219074085354805, at3_loss: 5.731526364627371e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
cls_loss: 0.1576872318983078, at3_loss: 5.689893001203927e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.255794495344162, at3_loss: 6.8833827526759706e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
cls_loss: 0.18759609758853912, at3_loss: 5.88418203051333e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
cls_loss: 0.2280586063861847, at3_loss: 4.565792188770956e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.1882866770029068, at3_loss: 4.1772141301521515e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
cls_loss: 0.24154742062091827, at3_loss: 7.0360384185619296e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.23409578204154968, at3_loss: 1.5459855617905305e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999987483024597, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988675117493, max: 1.0
cls_loss: 0.27166545391082764, at3_loss: 5.329070518200751e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999988079071045, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.23167142271995544, at3_loss: 1.1338152638984411e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999991655349731, max: 1.0
cls_loss: 0.1717665195465088, at3_loss: 4.274358644806853e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.21719728410243988, at3_loss: 6.494804694057166e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.1507009118795395, at3_loss: 4.912736883966318e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.22729048132896423, at3_loss: 4.565792188770956e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 1.0
cls_loss: 0.21627283096313477, at3_loss: 6.689093723366568e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.19118332862854004, at3_loss: 6.175615574477433e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999998152256012, max: 1.0
cls_loss: 0.15494300425052643, at3_loss: 1.3225531780847177e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.25036558508872986, at3_loss: 4.385380947269368e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999996423721313, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999990463256836, max: 1.0
cls_loss: 0.24436208605766296, at3_loss: 8.271161533457416e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999980926513672, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992251396179, max: 1.0
cls_loss: 0.27057987451553345, at3_loss: 1.659783421814609e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986886978149, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995231628418, max: 0.9999999403953552
cls_loss: 0.2721303403377533, at3_loss: 1.0061396160665481e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.1876813918352127, at3_loss: 8.743006318923108e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.1795208752155304, at3_loss: 1.199040866595169e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.18354089558124542, at3_loss: 6.897260540483785e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999977946281433, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
cls_loss: 0.22572574019432068, at3_loss: 2.235711615838909e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999983310699463, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.19981972873210907, at3_loss: 1.7666423879347803e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999997079372406, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999993443489075, max: 1.0
cls_loss: 0.2860325276851654, at3_loss: 2.8435587218211822e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999971985816956, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.22597798705101013, at3_loss: 4.0217829067046296e-11
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999994039535522, max: 1.0
cls_loss: 0.21493320167064667, at3_loss: 4.260480856999038e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999995827674866, max: 1.0
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999992847442627, max: 1.0
cls_loss: 0.17312517762184143, at3_loss: 6.189493362285248e-12
Calculating Loss
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.9999986290931702, max: 0.9999999403953552
Attention map shape: torch.Size([256, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.24243849515914917, at3_loss: 9.089951014118469e-12
Calculating Loss
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
cls_loss: 0.21162056922912598, at3_loss: 8.25265810550091e-12
Activation shape: torch.Size([96, 1536, 1, 1])
Activation3_s min: -0.21933899819850922, max: 0.6508954763412476
Activation3_s: tensor([[[[ 0.0842]],

         [[ 0.0148]],

         [[-0.0079]],

         ...,

         [[-0.0035]],

         [[-0.0030]],

         [[-0.0883]]],


        [[[ 0.1678]],

         [[-0.0570]],

         [[-0.0010]],

         ...,

         [[-0.0059]],

         [[ 0.0715]],

         [[-0.0797]]],


        [[[-0.0882]],

         [[ 0.1086]],

         [[ 0.0205]],

         ...,

         [[ 0.0189]],

         [[-0.0643]],

         [[ 0.1534]]],


        ...,


        [[[ 0.1680]],

         [[-0.0608]],

         [[ 0.0343]],

         ...,

         [[-0.0100]],

         [[ 0.0596]],

         [[-0.0340]]],


        [[[-0.0819]],

         [[ 0.1364]],

         [[-0.0942]],

         ...,

         [[ 0.0630]],

         [[-0.0417]],

         [[-0.0987]]],


        [[[-0.0957]],

         [[ 0.1395]],

         [[ 0.0559]],

         ...,

         [[ 0.0255]],

         [[-0.0944]],

         [[ 0.1989]]]], device='cuda:0', grad_fn=<MeanBackward1>)
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.9999991059303284, max: 0.9999999403953552
Attention map: tensor([[[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]]], device='cuda:0', grad_fn=<DivBackward0>)
Activation shape: torch.Size([96, 1536, 1, 1])
Activation3_t min: -0.19438385963439941, max: 0.7013508081436157
Activation3_t: tensor([[[[ 0.0581]],

         [[-0.0357]],

         [[ 0.0189]],

         ...,

         [[-0.0230]],

         [[-0.0409]],

         [[-0.0112]]],


        [[[ 0.1960]],

         [[-0.0385]],

         [[ 0.0401]],

         ...,

         [[-0.0232]],

         [[ 0.0515]],

         [[-0.0662]]],


        [[[-0.0150]],

         [[ 0.1593]],

         [[ 0.0120]],

         ...,

         [[ 0.0082]],

         [[-0.0125]],

         [[ 0.1631]]],


        ...,


        [[[ 0.1973]],

         [[-0.0332]],

         [[ 0.0688]],

         ...,

         [[-0.0239]],

         [[ 0.0709]],

         [[-0.0368]]],


        [[[-0.1039]],

         [[ 0.1538]],

         [[-0.0889]],

         ...,

         [[ 0.0868]],

         [[-0.0552]],

         [[-0.0924]]],


        [[[-0.0836]],

         [[ 0.1365]],

         [[ 0.0551]],

         ...,

         [[ 0.0216]],

         [[-0.0661]],

         [[ 0.1692]]]], device='cuda:0', grad_fn=<MeanBackward1>)
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map shape: torch.Size([96, 1, 1, 1])
Attention map min: 0.999999463558197, max: 1.0
Attention map: tensor([[[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]]], device='cuda:0', grad_fn=<DivBackward0>)
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:16:48 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
WARNING:root:zero len array in func all_acc(), return None!
2025-03-19:03:16:48 [WARNING ] [trainer_cls.py:590] zero len array in func all_acc(), return None!
INFO:root:Epoch9: Loss:18.678738936781883 Training Acc:91.56944444444444(19779/21600)
2025-03-19:03:16:48 [INFO    ] [nad.py:622] Epoch9: Loss:18.678738936781883 Training Acc:91.56944444444444(19779/21600)
features_out shape: [Sequential(
  (0): ConvNormActivation(
    (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): SiLU(inplace=True)
  )
  (1): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (2): ConvNormActivation(
          (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.0, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (2): ConvNormActivation(
          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)
    )
  )
  (2): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)
    )
    (2): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)
    )
  )
  (3): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)
    )
    (2): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)
    )
  )
  (4): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)
    )
    (2): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)
    )
    (3): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)
    )
    (4): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)
    )
  )
  (5): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.1, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)
    )
    (2): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)
    )
    (3): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)
    )
    (4): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)
    )
  )
  (6): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)
          (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)
    )
    (2): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)
    )
    (3): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)
    )
    (4): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)
    )
    (5): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)
    )
  )
  (7): Sequential(
    (0): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)
          (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)
    )
    (1): MBConv(
      (block): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): ConvNormActivation(
          (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
          (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (2): SqueezeExcitation(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))
          (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (scale_activation): Sigmoid()
        )
        (3): ConvNormActivation(
          (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)
    )
  )
  (8): ConvNormActivation(
    (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): SiLU(inplace=True)
  )
), AdaptiveAvgPool2d(output_size=1)]
INFO:root:{'batch': 0,
 'bd_test_loss_avg_over_batch': 5.503412502152579,
 'clean_test_loss_avg_over_batch': 1.3829659042936382,
 'epoch': 0,
 'test_acc': 0.6620238095238096,
 'test_asr': 0.073,
 'test_ra': 0.718,
 'train_acc': 0.9156944444444445,
 'train_epoch_loss_avg_over_batch': 0.21974986984449274}
2025-03-19:03:17:03 [INFO    ] [trainer_cls.py:65] {'batch': 0,
 'bd_test_loss_avg_over_batch': 5.503412502152579,
 'clean_test_loss_avg_over_batch': 1.3829659042936382,
 'epoch': 0,
 'test_acc': 0.6620238095238096,
 'test_asr': 0.073,
 'test_ra': 0.718,
 'train_acc': 0.9156944444444445,
 'train_epoch_loss_avg_over_batch': 0.21974986984449274}
INFO:root:saving...
2025-03-19:03:17:04 [INFO    ] [save_load_attack.py:176] saving...
